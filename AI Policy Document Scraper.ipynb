{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmQn1RmFYv9D"
   },
   "source": [
    "# **AI Policy Document Scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHWT1encYf-K",
    "outputId": "4d31c86f-fea0-46ad-dbbe-761feeedf3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Summary:\n",
      "{\n",
      "  \"total_documents\": 6,\n",
      "  \"documents_by_country\": {\n",
      "    \"eu\": 6\n",
      "  },\n",
      "  \"documents_by_type\": {\n",
      "    \"law\": 6\n",
      "  },\n",
      "  \"average_ai_relevance\": 1.0,\n",
      "  \"content_types\": {\n",
      "    \"html\": 6\n",
      "  },\n",
      "  \"scraping_date\": \"2025-07-24T18:31:16.138054\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI Policy Document Scraper\n",
    "Automated collection of AI-related policy documents from government sources\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "class PolicyScraper:\n",
    "    def __init__(self, config_path: str = \"config/data_sources.yaml\"):\n",
    "        \"\"\"Initialize the policy scraper with configuration\"\"\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Load configuration\n",
    "        self.config = self._load_config(config_path)\n",
    "\n",
    "        # AI-related keywords for document filtering\n",
    "        self.ai_keywords = [\n",
    "            'artificial intelligence', 'machine learning', 'deep learning',\n",
    "            'AI', 'ML', 'algorithmic', 'automated decision', 'neural network',\n",
    "            'intelligence artificielle', 'apprentissage automatique',  # French\n",
    "            'الذكاء الاصطناعي', 'التعلم الآلي'  # Arabic\n",
    "        ]\n",
    "\n",
    "    def _load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load configuration from file or return default config\"\"\"\n",
    "        default_config = {\n",
    "            \"sources\": {\n",
    "                \"algeria\": {\n",
    "                    \"base_url\": \"https://www.joradp.dz/\",\n",
    "                    \"search_patterns\": [\"/recherche\", \"/documents\"],\n",
    "                    \"language\": \"ar\"\n",
    "                },\n",
    "                \"eu\": {\n",
    "                    \"base_url\": \"https://digital-strategy.ec.europa.eu/\",\n",
    "                    \"search_patterns\": [\"/policies/artificial-intelligence\"],\n",
    "                    \"language\": \"en\"\n",
    "                },\n",
    "                \"usa\": {\n",
    "                    \"base_url\": \"https://www.whitehouse.gov/\",\n",
    "                    \"search_patterns\": [\"/briefing-room/presidential-actions\", \"/ai\"],\n",
    "                    \"language\": \"en\"\n",
    "                }\n",
    "            },\n",
    "            \"delay\": 2,  # seconds between requests\n",
    "            \"timeout\": 30\n",
    "        }\n",
    "\n",
    "        if os.path.exists(config_path):\n",
    "            try:\n",
    "                import yaml\n",
    "                with open(config_path, 'r') as file:\n",
    "                    return yaml.safe_load(file)\n",
    "            except:\n",
    "                self.logger.warning(f\"Could not load config from {config_path}, using defaults\")\n",
    "\n",
    "        return default_config\n",
    "\n",
    "    def scrape_government_sites(self, country_codes: List[str]) -> List[Dict]:\n",
    "        \"\"\"Scrape AI policies from government websites\"\"\"\n",
    "        all_documents = []\n",
    "\n",
    "        for country in country_codes:\n",
    "            if country not in self.config[\"sources\"]:\n",
    "                self.logger.warning(f\"No configuration found for country: {country}\")\n",
    "                continue\n",
    "\n",
    "            self.logger.info(f\"Scraping documents for {country}\")\n",
    "            country_docs = self._scrape_country_site(country)\n",
    "            all_documents.extend(country_docs)\n",
    "\n",
    "            # Respect rate limiting\n",
    "            time.sleep(self.config.get(\"delay\", 2))\n",
    "\n",
    "        return all_documents\n",
    "\n",
    "    def _scrape_country_site(self, country: str) -> List[Dict]:\n",
    "        \"\"\"Scrape documents from a specific country's government site\"\"\"\n",
    "        source_config = self.config[\"sources\"][country]\n",
    "        base_url = source_config[\"base_url\"]\n",
    "        documents = []\n",
    "\n",
    "        try:\n",
    "            # Get main page\n",
    "            response = self.session.get(base_url, timeout=self.config.get(\"timeout\", 30))\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find links that might contain AI-related documents\n",
    "            links = self._extract_relevant_links(soup, base_url, country)\n",
    "\n",
    "            # Process each link\n",
    "            for i, link in enumerate(links[:10]):  # Limit to first 10 links\n",
    "                try:\n",
    "                    doc_info = self._process_document_link(link, country)\n",
    "                    if doc_info:\n",
    "                        documents.append(doc_info)\n",
    "\n",
    "                    # Rate limiting\n",
    "                    if i % 3 == 0:\n",
    "                        time.sleep(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing link {link}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error scraping {country}: {str(e)}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _extract_relevant_links(self, soup: BeautifulSoup, base_url: str, country: str) -> List[str]:\n",
    "        \"\"\"Extract links that might contain AI-related documents\"\"\"\n",
    "        links = []\n",
    "\n",
    "        # Find all links on the page\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text().lower()\n",
    "\n",
    "            # Convert relative URLs to absolute\n",
    "            full_url = urljoin(base_url, href)\n",
    "\n",
    "            # Check if link text contains AI-related keywords\n",
    "            if any(keyword.lower() in text for keyword in self.ai_keywords):\n",
    "                links.append(full_url)\n",
    "\n",
    "            # Check for document extensions\n",
    "            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx']):\n",
    "                if any(keyword.lower() in text for keyword in self.ai_keywords):\n",
    "                    links.append(full_url)\n",
    "\n",
    "        return list(set(links))  # Remove duplicates\n",
    "\n",
    "    def _process_document_link(self, url: str, country: str) -> Optional[Dict]:\n",
    "        \"\"\"Process a single document link and extract metadata\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=self.config.get(\"timeout\", 30))\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Determine if it's a PDF or HTML document\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "\n",
    "            if 'pdf' in content_type:\n",
    "                return self._process_pdf_document(url, response.content, country)\n",
    "            else:\n",
    "                return self._process_html_document(url, response.text, country)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing document {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_html_document(self, url: str, content: str, country: str) -> Dict:\n",
    "        \"\"\"Process HTML document and extract metadata\"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('title')\n",
    "        title_text = title.get_text().strip() if title else \"Unknown Title\"\n",
    "\n",
    "        # Extract main content\n",
    "        main_content = self._extract_main_content(soup)\n",
    "\n",
    "        # Check if content is AI-related\n",
    "        if not self._is_ai_related(main_content):\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title_text,\n",
    "            'country': country,\n",
    "            'content_type': 'html',\n",
    "            'content_preview': main_content[:500] + \"...\" if len(main_content) > 500 else main_content,\n",
    "            'scraped_date': datetime.now().isoformat(),\n",
    "            'document_type': self.classify_document_type(title_text + \" \" + main_content),\n",
    "            'estimated_date': self._extract_date_from_content(main_content),\n",
    "            'word_count': len(main_content.split()),\n",
    "            'ai_relevance_score': self._calculate_ai_relevance(main_content)\n",
    "        }\n",
    "\n",
    "    def _process_pdf_document(self, url: str, content: bytes, country: str) -> Dict:\n",
    "        \"\"\"Process PDF document and extract metadata\"\"\"\n",
    "        # For PDF processing, we'll extract basic metadata\n",
    "        # In a full implementation, you'd use PyPDF2 or similar\n",
    "\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': self._extract_title_from_url(url),\n",
    "            'country': country,\n",
    "            'content_type': 'pdf',\n",
    "            'content_preview': \"PDF document - content extraction requires additional processing\",\n",
    "            'scraped_date': datetime.now().isoformat(),\n",
    "            'document_type': 'policy_document',  # Default for PDFs\n",
    "            'file_size': len(content),\n",
    "            'ai_relevance_score': 0.7  # Assume moderate relevance for PDFs found via AI keywords\n",
    "        }\n",
    "\n",
    "    def _extract_main_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract main content from HTML\"\"\"\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # Try to find main content areas\n",
    "        main_areas = soup.find_all(['main', 'article', 'div'], class_=re.compile(r'content|main|article'))\n",
    "\n",
    "        if main_areas:\n",
    "            content = ' '.join([area.get_text() for area in main_areas])\n",
    "        else:\n",
    "            # Fallback to body content\n",
    "            body = soup.find('body')\n",
    "            content = body.get_text() if body else soup.get_text()\n",
    "\n",
    "        # Clean up whitespace\n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        return content\n",
    "\n",
    "    def _is_ai_related(self, content: str) -> bool:\n",
    "        \"\"\"Check if content is AI-related\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        return any(keyword.lower() in content_lower for keyword in self.ai_keywords)\n",
    "\n",
    "    def _extract_date_from_content(self, content: str) -> Optional[str]:\n",
    "        \"\"\"Extract publication date from content\"\"\"\n",
    "        # Look for common date patterns\n",
    "        date_patterns = [\n",
    "            r'\\d{1,2}/\\d{1,2}/\\d{4}',  # MM/DD/YYYY or DD/MM/YYYY\n",
    "            r'\\d{4}-\\d{2}-\\d{2}',      # YYYY-MM-DD\n",
    "            r'\\d{1,2}\\s+\\w+\\s+\\d{4}',  # DD Month YYYY\n",
    "        ]\n",
    "\n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, content)\n",
    "            if match:\n",
    "                return match.group()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _extract_title_from_url(self, url: str) -> str:\n",
    "        \"\"\"Extract title from URL\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        filename = os.path.basename(parsed.path)\n",
    "\n",
    "        # Remove extension and clean up\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        title = title.replace('-', ' ').replace('_', ' ')\n",
    "        return title.title()\n",
    "\n",
    "    def _calculate_ai_relevance(self, content: str) -> float:\n",
    "        \"\"\"Calculate AI relevance score based on keyword frequency\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        total_words = len(content.split())\n",
    "\n",
    "        if total_words == 0:\n",
    "            return 0.0\n",
    "\n",
    "        ai_word_count = sum(content_lower.count(keyword.lower()) for keyword in self.ai_keywords)\n",
    "        relevance_score = min(ai_word_count / total_words * 100, 1.0)  # Cap at 1.0\n",
    "\n",
    "        return round(relevance_score, 3)\n",
    "\n",
    "    def classify_document_type(self, text: str) -> str:\n",
    "        \"\"\"Classify document type based on content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Define classification keywords\n",
    "        type_keywords = {\n",
    "            'law': ['act', 'law', 'regulation', 'statute', 'legal', 'loi', 'règlement'],\n",
    "            'strategy': ['strategy', 'plan', 'roadmap', 'framework', 'stratégie', 'plan'],\n",
    "            'guideline': ['guideline', 'guidance', 'recommendation', 'best practice', 'guide'],\n",
    "            'policy': ['policy', 'directive', 'politique', 'directive'],\n",
    "            'report': ['report', 'study', 'analysis', 'research', 'rapport', 'étude'],\n",
    "            'white_paper': ['white paper', 'position paper', 'livre blanc']\n",
    "        }\n",
    "\n",
    "        # Count matches for each type\n",
    "        type_scores = {}\n",
    "        for doc_type, keywords in type_keywords.items():\n",
    "            score = sum(text_lower.count(keyword) for keyword in keywords)\n",
    "            type_scores[doc_type] = score\n",
    "\n",
    "        # Return type with highest score, or 'unknown' if no matches\n",
    "        if max(type_scores.values()) > 0:\n",
    "            return max(type_scores, key=type_scores.get)\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    def save_results(self, documents: List[Dict], output_path: str = \"datasets/scraped_policies.json\"):\n",
    "        \"\"\"Save scraped documents to file\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # Also save as CSV for easier analysis\n",
    "        csv_path = output_path.replace('.json', '.csv')\n",
    "        df = pd.DataFrame(documents)\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "        self.logger.info(f\"Saved {len(documents)} documents to {output_path} and {csv_path}\")\n",
    "\n",
    "    def generate_summary_report(self, documents: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate summary statistics for scraped documents\"\"\"\n",
    "        if not documents:\n",
    "            return {\"error\": \"No documents to analyze\"}\n",
    "\n",
    "        df = pd.DataFrame(documents)\n",
    "\n",
    "        summary = {\n",
    "            \"total_documents\": len(documents),\n",
    "            \"documents_by_country\": df['country'].value_counts().to_dict(),\n",
    "            \"documents_by_type\": df['document_type'].value_counts().to_dict(),\n",
    "            \"average_ai_relevance\": df['ai_relevance_score'].mean() if 'ai_relevance_score' in df.columns else 0,\n",
    "            \"content_types\": df['content_type'].value_counts().to_dict(),\n",
    "            \"scraping_date\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize scraper\n",
    "    scraper = PolicyScraper()\n",
    "\n",
    "    # Scrape documents from specified countries\n",
    "    countries = ['algeria', 'eu', 'usa']\n",
    "    documents = scraper.scrape_government_sites(countries)\n",
    "\n",
    "    # Save results\n",
    "    scraper.save_results(documents)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = scraper.generate_summary_report(documents)\n",
    "    print(\"Scraping Summary:\")\n",
    "    print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
